{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7章: 単語ベクトル\n",
    "\n",
    "[https://nlp100.github.io/ja/ch07.html](https://nlp100.github.io/ja/ch07.html)\n",
    "\n",
    "## 60. 単語ベクトルの読み込みと表示 \n",
    "\n",
    "Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.61328125e-02, -4.83398438e-02,  2.35351562e-01,  1.74804688e-01,\n",
       "       -1.46484375e-01, -7.42187500e-02, -1.01562500e-01, -7.71484375e-02,\n",
       "        1.09375000e-01, -5.71289062e-02, -1.48437500e-01, -6.00585938e-02,\n",
       "        1.74804688e-01, -7.71484375e-02,  2.58789062e-02, -7.66601562e-02,\n",
       "       -3.80859375e-02,  1.35742188e-01,  3.75976562e-02, -4.19921875e-02,\n",
       "       -3.56445312e-02,  5.34667969e-02,  3.68118286e-04, -1.66992188e-01,\n",
       "       -1.17187500e-01,  1.41601562e-01, -1.69921875e-01, -6.49414062e-02,\n",
       "       -1.66992188e-01,  1.00585938e-01,  1.15722656e-01, -2.18750000e-01,\n",
       "       -9.86328125e-02, -2.56347656e-02,  1.23046875e-01, -3.54003906e-02,\n",
       "       -1.58203125e-01, -1.60156250e-01,  2.94189453e-02,  8.15429688e-02,\n",
       "        6.88476562e-02,  1.87500000e-01,  6.49414062e-02,  1.15234375e-01,\n",
       "       -2.27050781e-02,  3.32031250e-01, -3.27148438e-02,  1.77734375e-01,\n",
       "       -2.08007812e-01,  4.54101562e-02, -1.23901367e-02,  1.19628906e-01,\n",
       "        7.44628906e-03, -9.03320312e-03,  1.14257812e-01,  1.69921875e-01,\n",
       "       -2.38281250e-01, -2.79541016e-02, -1.21093750e-01,  2.47802734e-02,\n",
       "        7.71484375e-02, -2.81982422e-02, -4.71191406e-02,  1.78222656e-02,\n",
       "       -1.23046875e-01, -5.32226562e-02,  2.68554688e-02, -3.11279297e-02,\n",
       "       -5.59082031e-02, -5.00488281e-02, -3.73535156e-02,  1.25976562e-01,\n",
       "        5.61523438e-02,  1.51367188e-01,  4.29687500e-02, -2.08007812e-01,\n",
       "       -4.78515625e-02,  2.78320312e-02,  1.81640625e-01,  2.20703125e-01,\n",
       "       -3.61328125e-02, -8.39843750e-02, -3.69548798e-05, -9.52148438e-02,\n",
       "       -1.25000000e-01, -1.95312500e-01, -1.50390625e-01, -4.15039062e-02,\n",
       "        1.31835938e-01,  1.17675781e-01,  1.91650391e-02,  5.51757812e-02,\n",
       "       -9.42382812e-02, -1.08886719e-01,  7.32421875e-02, -1.15234375e-01,\n",
       "        8.93554688e-02, -1.40625000e-01,  1.45507812e-01,  4.49218750e-02,\n",
       "       -1.10473633e-02, -1.62353516e-02,  4.05883789e-03,  3.75976562e-02,\n",
       "       -6.98242188e-02, -5.46875000e-02,  2.17285156e-02, -9.47265625e-02,\n",
       "        4.24804688e-02,  1.81884766e-02, -1.73339844e-02,  4.63867188e-02,\n",
       "       -1.42578125e-01,  1.99218750e-01,  1.10839844e-01,  2.58789062e-02,\n",
       "       -7.08007812e-02, -5.54199219e-02,  3.45703125e-01,  1.61132812e-01,\n",
       "       -2.44140625e-01, -2.59765625e-01, -9.71679688e-02,  8.00781250e-02,\n",
       "       -8.78906250e-02, -7.22656250e-02,  1.42578125e-01, -8.54492188e-02,\n",
       "       -3.18359375e-01,  8.30078125e-02,  6.34765625e-02,  1.64062500e-01,\n",
       "       -1.92382812e-01, -1.17675781e-01, -5.41992188e-02, -1.56250000e-01,\n",
       "       -1.21582031e-01, -4.95605469e-02,  1.20117188e-01, -3.83300781e-02,\n",
       "        5.51757812e-02, -8.97216797e-03,  4.32128906e-02,  6.93359375e-02,\n",
       "        8.93554688e-02,  2.53906250e-01,  1.65039062e-01,  1.64062500e-01,\n",
       "       -1.41601562e-01,  4.58984375e-02,  1.97265625e-01, -8.98437500e-02,\n",
       "        3.90625000e-02, -1.51367188e-01, -8.60595703e-03, -1.17675781e-01,\n",
       "       -1.97265625e-01, -1.12792969e-01,  1.29882812e-01,  1.96289062e-01,\n",
       "        1.56402588e-03,  3.93066406e-02,  2.17773438e-01, -1.43554688e-01,\n",
       "        6.03027344e-02, -1.35742188e-01,  1.16210938e-01, -1.59912109e-02,\n",
       "        2.79296875e-01,  1.46484375e-01, -1.19628906e-01,  1.76757812e-01,\n",
       "        1.28906250e-01, -1.49414062e-01,  6.93359375e-02, -1.72851562e-01,\n",
       "        9.22851562e-02,  1.33056641e-02, -2.00195312e-01, -9.76562500e-02,\n",
       "       -1.65039062e-01, -2.46093750e-01, -2.35595703e-02, -2.11914062e-01,\n",
       "        1.84570312e-01, -1.85546875e-02,  2.16796875e-01,  5.05371094e-02,\n",
       "        2.02636719e-02,  4.25781250e-01,  1.28906250e-01, -2.77099609e-02,\n",
       "        1.29882812e-01, -1.15722656e-01, -2.05078125e-02,  1.49414062e-01,\n",
       "        7.81250000e-03, -2.05078125e-01, -8.05664062e-02, -2.67578125e-01,\n",
       "       -2.29492188e-02, -8.20312500e-02,  8.64257812e-02,  7.61718750e-02,\n",
       "       -3.66210938e-02,  5.22460938e-02, -1.22070312e-01, -1.44042969e-02,\n",
       "       -2.69531250e-01,  8.44726562e-02, -2.52685547e-02, -2.96630859e-02,\n",
       "       -1.68945312e-01,  1.93359375e-01, -1.08398438e-01,  1.94091797e-02,\n",
       "       -1.80664062e-01,  1.93359375e-01, -7.08007812e-02,  5.85937500e-02,\n",
       "       -1.01562500e-01, -1.31835938e-01,  7.51953125e-02, -7.66601562e-02,\n",
       "        3.37219238e-03, -8.59375000e-02,  1.25000000e-01,  2.92968750e-02,\n",
       "        1.70898438e-01, -9.37500000e-02, -1.09375000e-01, -2.50244141e-02,\n",
       "        2.11914062e-01, -4.44335938e-02,  6.12792969e-02,  2.62451172e-02,\n",
       "       -1.77734375e-01,  1.23046875e-01, -7.42187500e-02, -1.67968750e-01,\n",
       "       -1.08886719e-01, -9.04083252e-04, -7.37304688e-02,  5.49316406e-02,\n",
       "        6.03027344e-02,  8.39843750e-02,  9.17968750e-02, -1.32812500e-01,\n",
       "        1.22070312e-01, -8.78906250e-03,  1.19140625e-01, -1.94335938e-01,\n",
       "       -6.64062500e-02, -2.07031250e-01,  7.37304688e-02,  8.93554688e-02,\n",
       "        1.81884766e-02, -1.20605469e-01, -2.61230469e-02,  2.67333984e-02,\n",
       "        7.76367188e-02, -8.30078125e-02,  6.78710938e-02, -3.54003906e-02,\n",
       "        3.10546875e-01, -2.42919922e-02, -1.41601562e-01, -2.08007812e-01,\n",
       "       -4.57763672e-03, -6.54296875e-02, -4.95605469e-02,  2.22656250e-01,\n",
       "        1.53320312e-01, -1.38671875e-01, -5.24902344e-02,  4.24804688e-02,\n",
       "       -2.38281250e-01,  1.56250000e-01,  5.83648682e-04, -1.20605469e-01,\n",
       "       -9.22851562e-02, -4.44335938e-02,  3.61328125e-02, -1.86767578e-02,\n",
       "       -8.25195312e-02, -8.25195312e-02, -4.05273438e-02,  1.19018555e-02,\n",
       "        1.69921875e-01, -2.80761719e-02,  3.03649902e-03,  9.32617188e-02,\n",
       "       -8.49609375e-02,  1.57470703e-02,  7.03125000e-02,  1.62353516e-02,\n",
       "       -2.27050781e-02,  3.51562500e-02,  2.47070312e-01, -2.67333984e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model_path = './GoogleNews-vectors-negative300.bin'\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "United_States = w2v['United_States']\n",
    "United_States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 61. 単語の類似度\n",
    "\n",
    "“United States”と”U.S.”のコサイン類似度を計算せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7310775]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "US = w2v['U.S.']\n",
    "cosine_similarity(United_States.reshape(1, -1), US.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 62. 類似度の高い単語10件\n",
    "\n",
    "“United States”とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Unites_States', 0.7877248525619507)\n",
      "('Untied_States', 0.7541370391845703)\n",
      "('United_Sates', 0.74007248878479)\n",
      "('U.S.', 0.7310774326324463)\n",
      "('theUnited_States', 0.6404393911361694)\n",
      "('America', 0.6178410053253174)\n",
      "('UnitedStates', 0.6167311668395996)\n",
      "('Europe', 0.6132988929748535)\n",
      "('countries', 0.6044804453849792)\n",
      "('Canada', 0.6019070148468018)\n"
     ]
    }
   ],
   "source": [
    "results = w2v.most_similar(positive=['United_States'])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 63. 加法構成性によるアナロジー\n",
    "\n",
    "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Greece', 0.6898481249809265)\n",
      "('Aristeidis_Grigoriadis', 0.560684859752655)\n",
      "('Ioannis_Drymonakos', 0.5552908778190613)\n",
      "('Greeks', 0.545068621635437)\n",
      "('Ioannis_Christou', 0.5400862693786621)\n",
      "('Hrysopiyi_Devetzi', 0.5248444676399231)\n",
      "('Heraklio', 0.5207759737968445)\n",
      "('Athens_Greece', 0.516880989074707)\n",
      "('Lithuania', 0.5166865587234497)\n",
      "('Iraklion', 0.5146791934967041)\n"
     ]
    }
   ],
   "source": [
    "results = w2v.most_similar(positive=['Spain', 'Athens'], negative=['Madrid'])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64. アナロジーデータでの実験\n",
    "\n",
    "[単語アナロジーの評価データ](http://download.tensorflow.org/data/questions-words.txt)をダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19559/19559 [54:06<00:00,  6.02it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['Athens', 'Greece', 'Baghdad', 'Iraq', ('Iraqi', 0.6351870894432068)],\n",
       "  ['Athens',\n",
       "   'Greece',\n",
       "   'Bangkok',\n",
       "   'Thailand',\n",
       "   ('Thailand', 0.7137669920921326)],\n",
       "  ['Athens', 'Greece', 'Beijing', 'China', ('China', 0.7235777974128723)]],\n",
       " [['Abuja', 'Nigeria', 'Accra', 'Ghana', ('Ghana', 0.820812463760376)],\n",
       "  ['Abuja', 'Nigeria', 'Algiers', 'Algeria', ('Algeria', 0.6122236847877502)],\n",
       "  ['Abuja',\n",
       "   'Nigeria',\n",
       "   'Amman',\n",
       "   'Jordan',\n",
       "   ('Saudi_Arabia', 0.5893203020095825)]],\n",
       " [['Algeria', 'dinar', 'Angola', 'kwanza', ('kwanza', 0.5409914255142212)],\n",
       "  ['Algeria', 'dinar', 'Argentina', 'peso', ('peso', 0.5536295175552368)],\n",
       "  ['Algeria', 'dinar', 'Armenia', 'dram', ('hryvnia', 0.5733274221420288)]],\n",
       " [['Chicago', 'Illinois', 'Houston', 'Texas', ('Texas', 0.6967873573303223)],\n",
       "  ['Chicago',\n",
       "   'Illinois',\n",
       "   'Philadelphia',\n",
       "   'Pennsylvania',\n",
       "   ('Pennsylvania', 0.7848097085952759)],\n",
       "  ['Chicago',\n",
       "   'Illinois',\n",
       "   'Phoenix',\n",
       "   'Arizona',\n",
       "   ('Arizona', 0.6820878386497498)]],\n",
       " [['boy', 'girl', 'brother', 'sister', ('sister', 0.8177284002304077)],\n",
       "  ['boy', 'girl', 'brothers', 'sisters', ('sisters', 0.7985787391662598)],\n",
       "  ['boy', 'girl', 'dad', 'mom', ('mom', 0.7781193256378174)]],\n",
       " [['amazing',\n",
       "   'amazingly',\n",
       "   'apparent',\n",
       "   'apparently',\n",
       "   ('apparently', 0.48172980546951294)],\n",
       "  ['amazing', 'amazingly', 'calm', 'calmly', ('Calm', 0.5576373934745789)],\n",
       "  ['amazing',\n",
       "   'amazingly',\n",
       "   'cheerful',\n",
       "   'cheerfully',\n",
       "   ('irrepressibly', 0.5931416749954224)]],\n",
       " [['acceptable',\n",
       "   'unacceptable',\n",
       "   'aware',\n",
       "   'unaware',\n",
       "   ('unaware', 0.6383267045021057)],\n",
       "  ['acceptable',\n",
       "   'unacceptable',\n",
       "   'certain',\n",
       "   'uncertain',\n",
       "   ('unjustified', 0.46223652362823486)],\n",
       "  ['acceptable',\n",
       "   'unacceptable',\n",
       "   'clear',\n",
       "   'unclear',\n",
       "   ('crystal_clear', 0.4839097857475281)]],\n",
       " [['bad', 'worse', 'big', 'bigger', ('bigger', 0.7387213110923767)],\n",
       "  ['bad', 'worse', 'bright', 'brighter', ('brighter', 0.6739344000816345)],\n",
       "  ['bad', 'worse', 'cheap', 'cheaper', ('cheaper', 0.6605448722839355)]],\n",
       " [['bad', 'worst', 'big', 'biggest', ('biggest', 0.7630176544189453)],\n",
       "  ['bad', 'worst', 'bright', 'brightest', ('brightest', 0.5385545492172241)],\n",
       "  ['bad', 'worst', 'cold', 'coldest', ('coldest', 0.6185534000396729)]],\n",
       " [['code', 'coding', 'dance', 'dancing', ('dancing', 0.6104243993759155)],\n",
       "  ['code', 'coding', 'debug', 'debugging', ('debugging', 0.6630100011825562)],\n",
       "  ['code',\n",
       "   'coding',\n",
       "   'decrease',\n",
       "   'decreasing',\n",
       "   ('decreases', 0.5992642641067505)]],\n",
       " [['Albania',\n",
       "   'Albanian',\n",
       "   'Argentina',\n",
       "   'Argentinean',\n",
       "   ('Argentine', 0.7197668552398682)],\n",
       "  ['Albania',\n",
       "   'Albanian',\n",
       "   'Australia',\n",
       "   'Australian',\n",
       "   ('Australian', 0.7241904735565186)],\n",
       "  ['Albania',\n",
       "   'Albanian',\n",
       "   'Austria',\n",
       "   'Austrian',\n",
       "   ('Austrian', 0.7370085120201111)]],\n",
       " [['dancing',\n",
       "   'danced',\n",
       "   'decreasing',\n",
       "   'decreased',\n",
       "   ('increasing', 0.6025133728981018)],\n",
       "  ['dancing',\n",
       "   'danced',\n",
       "   'describing',\n",
       "   'described',\n",
       "   ('described', 0.6338616609573364)],\n",
       "  ['dancing',\n",
       "   'danced',\n",
       "   'enhancing',\n",
       "   'enhanced',\n",
       "   ('enhance', 0.5993480682373047)]],\n",
       " [['banana', 'bananas', 'bird', 'birds', ('birds', 0.7513800859451294)],\n",
       "  ['banana', 'bananas', 'bottle', 'bottles', ('bottles', 0.7688297033309937)],\n",
       "  ['banana',\n",
       "   'bananas',\n",
       "   'building',\n",
       "   'buildings',\n",
       "   ('buildings', 0.6134210228919983)]],\n",
       " [['decrease',\n",
       "   'decreases',\n",
       "   'describe',\n",
       "   'describes',\n",
       "   ('describing', 0.5969001054763794)],\n",
       "  ['decrease', 'decreases', 'eat', 'eats', ('eats', 0.6888678669929504)],\n",
       "  ['decrease',\n",
       "   'decreases',\n",
       "   'enhance',\n",
       "   'enhances',\n",
       "   ('enhances', 0.7360528111457825)]]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open('questions-words.txt', mode='rt', encoding='utf-8') as f:\n",
    "    read_data = f.read()\n",
    "\n",
    "analogies = {}\n",
    "current_analogy = \"\"\n",
    "\n",
    "for each in tqdm(read_data.split('\\n')):\n",
    "    sep_each = each.split(\" \")\n",
    "    \n",
    "    if len(sep_each) == 4:\n",
    "        sep_each.append(w2v.most_similar(positive=[sep_each[1], sep_each[2]], negative=[sep_each[0]], topn=1)[0])\n",
    "        analogies[current_analogy].append(sep_each)\n",
    "        \n",
    "    elif len(sep_each) < 4 and sep_each[0] == \":\":\n",
    "        current_analogy = sep_each[1]\n",
    "        analogies[current_analogy] = []\n",
    "\n",
    "[analogies[key][:3] for key in analogies.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 65. アナロジータスクでの正解率\n",
    "\n",
    "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of semantic analogy : 0.731\n",
      "Accuracy of syntactic analogy: 0.740\n"
     ]
    }
   ],
   "source": [
    "semantic_analogies = [\"capital-common-countries\", \"capital-world\", \"currency\", \"city-in-state\", \"family\"]\n",
    "syntactic_analogies = [\"gram1-adjective-to-adverb\", \"gram2-opposite\", \"gram3-comparative\", \n",
    "                       \"gram4-superlative\", \"gram5-present-participle\", \"gram6-nationality-adjective\", \n",
    "                       \"gram7-past-tense\", \"gram8-plural\", \"gram9-plural-verbs\"]\n",
    "\n",
    "semantic_acc, syntactic_acc = [], []\n",
    "\n",
    "for semantic_analogy in semantic_analogies:\n",
    "    for each in analogies[semantic_analogy]:\n",
    "        if each[3] == each[4][0]:\n",
    "            semantic_acc.append(1)\n",
    "        else:\n",
    "            semantic_acc.append(0)\n",
    "\n",
    "for syntactic_analogy in syntactic_analogies:\n",
    "    for each in analogies[syntactic_analogy]:\n",
    "        if each[3] == each[4][0]:\n",
    "            syntactic_acc.append(1)\n",
    "        else:\n",
    "            syntactic_acc.append(0)\n",
    "\n",
    "print('Accuracy of semantic analogy : {:.3f}'.format(sum(semantic_acc) / len(semantic_acc)))\n",
    "print('Accuracy of syntactic analogy: {:.3f}'.format(sum(syntactic_acc) / len(syntactic_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 66. WordSimilarity-353での評価\n",
    "\n",
    "[The WordSimilarity-353 Test Collectionの評価データ](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)をダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．\n",
    "\n",
    "\n",
    "### スピアマンの順位相関\n",
    "\n",
    "スピアマンの相関は、2つの連続変数または順位変数間の単調関係を評価します。単調関係では変数が一緒に変化しますが、一定の割合とは限りません。スピアマンの相関係数は、生データではなく各変数の順位値に基づきます。\n",
    "\n",
    "[出典：https://support.minitab.com/ja-jp/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/correlation-and-covariance/a-comparison-of-the-pearson-and-spearman-correlation-methods/](https://support.minitab.com/ja-jp/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/correlation-and-covariance/a-comparison-of-the-pearson-and-spearman-correlation-methods/9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6999720642348002, pvalue=2.9311328631751965e-53)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "ws_353 = pd.read_csv('wordsim353/combined.csv')\n",
    "cos_sims = []\n",
    "\n",
    "for pair in ws_353.itertuples():\n",
    "    word1_v, word2_v = w2v[pair[1]], w2v[pair[2]]\n",
    "    similarity = cosine_similarity(word1_v.reshape(1, -1), word2_v.reshape(1, -1)).item()\n",
    "    cos_sims.append([pair[1], pair[2], similarity])\n",
    "\n",
    "cos_sim_df = pd.DataFrame(cos_sims, columns=['Word 1', 'Word 2', 'Cosine similarity'])\n",
    "\n",
    "# assertion\n",
    "assert_frame_equal(ws_353[['Word 1', 'Word 2']], cos_sim_df[['Word 1', 'Word 2']])\n",
    "\n",
    "rank_by_human = ws_353.rank(numeric_only=True, method='first', ascending=False)['Human (mean)'].tolist()\n",
    "rank_by_cossim = cos_sim_df.rank(numeric_only=True, method='first', ascending=False)['Cosine similarity'].tolist()\n",
    "spearmanr(rank_by_human, rank_by_cossim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p値は2つのデータセットが無相関であるという帰無仮説についての両側p値。有意水準1%としたとき、この帰無仮説は棄却される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 67. k-meansクラスタリング\n",
    "\n",
    "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 68. Ward法によるクラスタリング\n",
    "\n",
    "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 69. t-SNEによる可視化\n",
    "\n",
    "ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
