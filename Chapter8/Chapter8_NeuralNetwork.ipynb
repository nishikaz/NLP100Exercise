{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python389jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.8.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 第8章: ニューラルネット\n",
    "\n",
    "[https://nlp100.github.io/ja/ch08.html](https://nlp100.github.io/ja/ch08.html)\n",
    "\n",
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 70. 単語ベクトルの和による特徴量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "# global variables\n",
    "dataset_types = ['train', 'valid', 'test']\n",
    "label2int = {\n",
    "    \"b\": 0,\n",
    "    \"t\": 1,\n",
    "    \"e\": 2,\n",
    "    \"m\": 3\n",
    "}\n",
    "Xs, ys = {}, {}\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using {} device'.format(device.type))\n",
    "\n",
    "\n",
    "def makeDatasetFiles():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Word2Vec\n",
    "    w2v = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        '../Chapter7/GoogleNews-vectors-negative300.bin', \n",
    "        binary=True)\n",
    "\n",
    "    for dataset in dataset_types:\n",
    "        tmp_x, tmp_y = [], []\n",
    "        tmp_df = pd.read_table('../Chapter6/{:}.txt'.format(dataset))\n",
    "\n",
    "        for each in tmp_df.itertuples():\n",
    "\n",
    "            # make X\n",
    "            tokens = [token for token in nlp(each.TITLE)]\n",
    "            num_tokens = len(tokens)\n",
    "\n",
    "            x_i = np.zeros(300)\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    token_embedding = w2v[str(token)]\n",
    "                    x_i = np.add(x_i, token_embedding)\n",
    "\n",
    "                except KeyError:\n",
    "                    num_tokens -= 1\n",
    "                    continue\n",
    "\n",
    "            x_i = np.divide(x_i, num_tokens)\n",
    "            tmp_x.append(x_i)\n",
    "\n",
    "            # make y\n",
    "            tmp_y.append(label2int[each.CATEGORY])\n",
    "        \n",
    "        # convert to torch.Tensor\n",
    "        Xs[dataset] = torch.Tensor([tmp_x]).float()\n",
    "        ys[dataset] = torch.Tensor([tmp_y]).long()\n",
    "\n",
    "        # pickle\n",
    "        torch.save(Xs[dataset], 'X_{:}.pickle'.format(dataset))\n",
    "        torch.save(ys[dataset], 'y_{:}.pickle'.format(dataset))\n",
    "    \n",
    "    return Xs, ys"
   ]
  },
  {
   "source": [
    "## 71. 単層ニューラルネットワークによる予測"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "try:\n",
    "    for dataset in dataset_types:\n",
    "        Xs[dataset] = torch.load(\n",
    "            'X_{:}.pickle'.format(dataset),\n",
    "            map_location=device)\n",
    "        ys[dataset] = torch.load(\n",
    "            'y_{:}.pickle'.format(dataset),\n",
    "            map_location=device)\n",
    "except FileNotFoundError:\n",
    "    Xs, ys = makeDatasetFiles()\n",
    "    assert Xs != {} and ys != {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hat_y_1 = tensor([[[0.2525, 0.2487, 0.2648, 0.2341]]])\nhat_Y = [tensor([[[0.2525, 0.2487, 0.2648, 0.2341]]]), tensor([[[0.2490, 0.2528, 0.2500, 0.2482]]]), tensor([[[0.2359, 0.2576, 0.2604, 0.2460]]]), tensor([[[0.2497, 0.2551, 0.2544, 0.2408]]])]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define network\n",
    "torch.manual_seed(46)\n",
    "single_layer_network = nn.Sequential(\n",
    "    nn.Linear(300, 4)\n",
    ")\n",
    "\n",
    "hat_Y, x_i, logits, probs = [], [], [], []\n",
    "for i in range(4):\n",
    "    x_i.append(torch.index_select(Xs['train'], dim=1, index=torch.tensor([i])))\n",
    "    logits.append(single_layer_network(x_i[-1]))\n",
    "    probs.append(F.softmax(logits[-1], dim=2))\n",
    "    hat_Y.append(probs[-1].detach())\n",
    "\n",
    "print('hat_y_1 = {:}'.format(hat_Y[0]))\n",
    "print('hat_Y = {:}'.format(hat_Y))"
   ]
  },
  {
   "source": [
    "## 72. 損失と勾配の計算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Losses ===========\nx_1 loss = tensor([[[1.3289]]], grad_fn=<NegBackward>)\naverage loss = 1.3831130266189575\n=== Grads ===========\ntensor([[ 0.0302,  0.0053, -0.0058,  ..., -0.0229, -0.0055, -0.0250],\n        [ 0.0298,  0.0053, -0.0057,  ..., -0.0225, -0.0055, -0.0246],\n        [-0.0881, -0.0156,  0.0168,  ...,  0.0666,  0.0162,  0.0727],\n        [ 0.0280,  0.0050, -0.0054,  ..., -0.0212, -0.0051, -0.0231]])\ntensor([[-0.0532,  0.0070,  0.0091,  ..., -0.0758, -0.0147,  0.0010],\n        [ 0.0579,  0.0047, -0.0107,  ..., -0.0047, -0.0024, -0.0333],\n        [-0.0603, -0.0161,  0.0119,  ...,  0.0842,  0.0192,  0.0640],\n        [ 0.0556,  0.0044, -0.0103,  ..., -0.0037, -0.0021, -0.0317]])\ntensor([[-0.0126, -0.0036,  0.0129,  ..., -0.1097, -0.0855, -0.0159],\n        [ 0.0442,  0.0083, -0.0120,  ...,  0.0067,  0.0215, -0.0276],\n        [-0.0741, -0.0125,  0.0106,  ...,  0.0957,  0.0433,  0.0698],\n        [ 0.0425,  0.0078, -0.0115,  ...,  0.0072,  0.0207, -0.0263]])\ntensor([[-0.0080, -0.0007,  0.0022,  ..., -0.1139, -0.0879, -0.0106],\n        [ 0.0488,  0.0113, -0.0229,  ...,  0.0024,  0.0191, -0.0222],\n        [-0.0877, -0.0213,  0.0426,  ...,  0.1085,  0.0505,  0.0540],\n        [ 0.0469,  0.0107, -0.0218,  ...,  0.0031,  0.0184, -0.0212]])\n"
     ]
    }
   ],
   "source": [
    "targets, losses = [], []\n",
    "for i in range(4):\n",
    "    targets.append(int(torch.index_select(ys['train'], dim=1, index=torch.tensor([i]))))\n",
    "    losses.append(-torch.log(torch.index_select(probs[i], dim=2, index=torch.tensor([targets[-1]]))))\n",
    "\n",
    "print('=== Losses ===========')\n",
    "print('x_1 loss = {:}'.format(losses[0]))\n",
    "print('average loss = {:}'.format(torch.mean(torch.Tensor(losses))))\n",
    "\n",
    "print('=== Grads ===========')\n",
    "for loss_ in losses:\n",
    "    loss_.backward()\n",
    "    print(single_layer_network[0].weight.grad)"
   ]
  },
  {
   "source": [
    "## 73. 確率的勾配降下法による学習"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 336.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(single_layer_network.parameters(), lr=0.1)\n",
    "X, y = Xs['train'], ys['train']\n",
    "\n",
    "for epoch in tqdm.tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    logits = single_layer_network(X)\n",
    "    loss = cross_entropy_loss(logits.squeeze(), y.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}