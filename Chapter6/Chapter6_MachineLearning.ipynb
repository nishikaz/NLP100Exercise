{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章: 機械学習\n",
    "本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. データの入手・整形\n",
    "\n",
    "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "3. 抽出された事例をランダムに並び替える．\n",
    "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "news_corpora = \"NewsAggregatorDataset/newsCorpora.csv\"\n",
    "columns = [\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "\n",
    "df = pd.read_csv(news_corpora, delimiter=\"\\t\", header=None, index_col=0, names=columns)\n",
    "\n",
    "target_publisher = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "df = df[df[\"PUBLISHER\"].isin(target_publisher)]\n",
    "\n",
    "# https://stackoverflow.com/questions/15772009/shuffling-permutating-a-dataframe-in-pandas\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "df_size = len(df)\n",
    "train_size, valid_test_size = int(df_size*0.8), int(df_size*0.1)\n",
    "assert df_size == train_size + valid_test_size * 2\n",
    "\n",
    "df_train = df.iloc[:train_size].reset_index(drop=True)\n",
    "df_valid = df.iloc[train_size:train_size+valid_test_size].reset_index(drop=True)\n",
    "df_test = df.iloc[train_size+valid_test_size:].reset_index(drop=True)\n",
    "\n",
    "df_train[[\"CATEGORY\", \"TITLE\"]].to_csv(\"train.txt\", index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
    "df_valid[[\"CATEGORY\", \"TITLE\"]].to_csv(\"valid.txt\", index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
    "df_test[[\"CATEGORY\", \"TITLE\"]].to_csv(\"test.txt\", index=False, sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10672, 7), (1334, 7), (1334, 7))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51. 特徴量抽出\n",
    "\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "publisher2int, int2publisher = {}, {}\n",
    "intpublisher = []\n",
    "\n",
    "# https://universaldependencies.org/docs/u/pos/\n",
    "open_class_words = [\"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publisher をカテゴリ変数化\n",
    "def makeCategoricalPublisher(dataseries):\n",
    "    for publisher in dataseries:\n",
    "        if publisher not in publisher2int.keys():\n",
    "            int2publisher[len(publisher2int)] = publisher\n",
    "            publisher2int[publisher] = len(publisher2int)\n",
    "        intpublisher.append(publisher2int[publisher])\n",
    "        \n",
    "    return intpublisher, int2publisher, publisher2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力された Title の DataSeries から Vocabulary を作成\n",
    "def makeVocab(dataseries, threshold=10):\n",
    "    vocab2int, int2vocab = {}, {}\n",
    "    vocab_count = {}\n",
    "    \n",
    "    for token in [token for title in dataseries for token in nlp(title.lower())]:\n",
    "        if token.pos_ in open_class_words:\n",
    "            if token.lemma_ not in vocab_count.keys():\n",
    "                vocab_count[token.lemma_] = 1\n",
    "            else:\n",
    "                vocab_count[token.lemma_] += 1\n",
    "\n",
    "    index = 0    \n",
    "    for word, count in vocab_count.items():\n",
    "        if count > threshold:\n",
    "            vocab2int[word] = index\n",
    "            int2vocab[index] = word\n",
    "            index += 1\n",
    "\n",
    "    return vocab2int, int2vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words を作る\n",
    "def makeBOW(dataseries, vocab2int):\n",
    "    bow = []\n",
    "    \n",
    "    for title in dataseries:\n",
    "        current_bag = [0] * (len(vocab2int) + 1)\n",
    "\n",
    "        for token in nlp(title.lower()):\n",
    "            if token.pos_ in open_class_words:\n",
    "                if token.lemma_ in vocab2int.keys():\n",
    "                    current_bag[vocab2int[token.lemma_]] += 1\n",
    "                else:\n",
    "                    current_bag[-1] += 1\n",
    "\n",
    "        bow.append(current_bag)\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intpublisher, int2publisher, publisher2int = makeCategoricalPublisher(df_train[\"PUBLISHER\"])\n",
    "vocab2int, int2vocab = makeVocab(df_train[\"TITLE\"], 15)\n",
    "bow = makeBOW(df_train[\"TITLE\"], vocab2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
