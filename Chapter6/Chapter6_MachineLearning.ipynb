{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章: 機械学習\n",
    "本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. データの入手・整形\n",
    "\n",
    "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "3. 抽出された事例をランダムに並び替える．\n",
    "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "news_corpora = \"NewsAggregatorDataset/newsCorpora.csv\"\n",
    "columns = [\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "\n",
    "df = pd.read_csv(news_corpora, delimiter=\"\\t\", header=None, index_col=0, names=columns)\n",
    "\n",
    "target_publisher = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "df = df[df[\"PUBLISHER\"].isin(target_publisher)]\n",
    "\n",
    "# https://stackoverflow.com/questions/15772009/shuffling-permutating-a-dataframe-in-pandas\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "df_size = len(df)\n",
    "train_size, valid_test_size = int(df_size*0.8), int(df_size*0.1)\n",
    "assert df_size == train_size + valid_test_size * 2\n",
    "\n",
    "df_train = df.iloc[:train_size].reset_index(drop=True)\n",
    "df_valid = df.iloc[train_size:train_size+valid_test_size].reset_index(drop=True)\n",
    "df_test = df.iloc[train_size+valid_test_size:].reset_index(drop=True)\n",
    "\n",
    "df_train[[\"CATEGORY\", \"TITLE\"]].to_csv(\"train.txt\", index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
    "df_valid[[\"CATEGORY\", \"TITLE\"]].to_csv(\"valid.txt\", index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
    "df_test[[\"CATEGORY\", \"TITLE\"]].to_csv(\"test.txt\", index=False, sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10672, 7), (1334, 7), (1334, 7))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_valid.shape, df_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
